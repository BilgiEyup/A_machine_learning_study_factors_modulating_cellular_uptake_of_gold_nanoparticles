{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M2tUSlB_F4L"
      },
      "outputs": [],
      "source": [
        "!pip install catboost\n",
        "!pip install pygam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The code snippet below is to get permutation importance graphs of both train and test sets individually for the defined models to plot. You can change the models_to_plot variable according to your model or define a sorting criteria to choose the models to be graphed."
      ],
      "metadata": {
        "id": "8lRPJ3vJH0BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # Importing pandas library for data manipulation and analysis\n",
        "import numpy as np  # Importing numpy library for numerical operations\n",
        "import warnings  # Importing warnings to manage warnings during the runtime\n",
        "import os  # Importing os for operating system dependent functionalities\n",
        "\n",
        "# Importing necessary classes and functions from sklearn for model building, preprocessing, and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "import matplotlib.pyplot as plt  # Importing matplotlib for plotting graphs\n",
        "\n",
        "# Loading the dataset\n",
        "data = pd.read_csv('path_to_your_data')\n",
        "\n",
        "# Transforming the target variable by applying a logarithmic function to make the distribution more symmetric\n",
        "data['Target_log'] = np.log(data['Target'] + 1)\n",
        "\n",
        "# Data preparation by dropping irrelevant columns\n",
        "X = data.drop(columns=['List_of_columns_to_drop'])\n",
        "y = data['Target_log']  # Specifying the target variable\n",
        "\n",
        "# Encoding categorical variables\n",
        "for col in X.select_dtypes(include='object').columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Best parameters\n",
        "rf_best_params = {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
        "xgb_best_params = {'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
        "bagging_best_params = {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 30}\n",
        "dt_best_params = {'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_leaf': 2, 'min_samples_split': 2, 'splitter': 'best'}\n",
        "\n",
        "# Create a dictionary of models\n",
        "regressors = {\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=None, min_samples_leaf=1, min_samples_split=2, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
        "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
        "    'Bagging': BaggingRegressor(n_estimators=30, max_features=1.0, max_samples=1.0, random_state=42),\n",
        "    'Decision Tree': DecisionTreeRegressor(criterion='friedman_mse', max_depth=9, min_samples_leaf=2, min_samples_split=2, splitter='best', random_state=42),\n",
        "    'Support Vector': SVR(),\n",
        "    'K-Neighbors': KNeighborsRegressor(),\n",
        "    'RidgeCV': RidgeCV(),\n",
        "    'Lasso': Lasso(),\n",
        "    'ElasticNet': ElasticNet(),\n",
        "    'SGD': SGDRegressor(),\n",
        "    'Gaussian Process': GaussianProcessRegressor(),\n",
        "    'BayesianRidge': BayesianRidge(),\n",
        "    'XGBoostRegressor': XGBRegressor(colsample_bytree=0.9, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8, random_state=42),\n",
        "    'LightGBMRegressor': LGBMRegressor(),\n",
        "    'SVR_poly': SVR(kernel='poly'),\n",
        "    'SVR_sigmoid': SVR(kernel='sigmoid'),\n",
        "    'SVR_rbf': SVR(kernel='rbf'),\n",
        "    'HuberRegressor': HuberRegressor(),\n",
        "    'RANSACRegressor': RANSACRegressor(),\n",
        "    'TheilSenRegressor': TheilSenRegressor(),\n",
        "    'KernelRidge': KernelRidge(),\n",
        "    'OMP': OrthogonalMatchingPursuit(),\n",
        "    'PoissonRegressor': PoissonRegressor(),\n",
        "    'TweedieRegressor': TweedieRegressor()\n",
        "}\n",
        "# Fit models and extract permutation importances\n",
        "perm_importances = {}  # Initializing the dictionary\n",
        "def plot_permutation_importance(model, X_train, y_train, X_test, y_test, title='', save_path=''):\n",
        "    # Compute permutation importance for training set\n",
        "    result_train = permutation_importance(model, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2)\n",
        "    sorted_idx_train = result_train.importances_mean.argsort()\n",
        "\n",
        "    # Compute permutation importance for test set\n",
        "    result_test = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
        "    sorted_idx_test = result_test.importances_mean.argsort()\n",
        "\n",
        "    # Convert the results to DataFrame for better visualization\n",
        "    importances_train = pd.DataFrame(result_train.importances[sorted_idx_train].T, columns=X.columns[sorted_idx_train])\n",
        "    importances_test = pd.DataFrame(result_test.importances[sorted_idx_test].T, columns=X.columns[sorted_idx_test])\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 12))\n",
        "\n",
        "    # Plotting for training set\n",
        "    importances_train.plot.box(vert=False, whis=10, ax=ax[0])\n",
        "    ax[0].set_title(f\"Permutation Importances (train set) - {title}\")\n",
        "    ax[0].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
        "    ax[0].set_xlabel(\"Decrease in accuracy score\")\n",
        "\n",
        "    # Plotting for test set\n",
        "    importances_test.plot.box(vert=False, whis=10, ax=ax[1])\n",
        "    ax[1].set_title(f\"Permutation Importances (test set) - {title}\")\n",
        "    ax[1].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
        "    ax[1].set_xlabel(\"Decrease in accuracy score\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    if save_path:\n",
        "        png_path = os.path.join(save_path, f\"{title}_Permutation_Importance.png\")\n",
        "        tiff_path = os.path.join(save_path, f\"{title}_Permutation_Importance.tiff\")\n",
        "#decrease the dpi if the resolution is larger than it should be\n",
        "        fig.savefig(png_path, dpi=1000)\n",
        "        fig.savefig(tiff_path, dpi=1000)\n",
        "\n",
        "    plt.close(fig)  # close the figure\n",
        "\n",
        "# Create directories if they don't exist\n",
        "save_directory = \"path_to_save_directory\"\n",
        "if not os.path.exists(save_directory):\n",
        "    os.makedirs(save_directory)\n",
        "    # Models to visualize\n",
        "models_to_plot = ['XGBoostRegressor', 'Random Forest', 'Bagging', 'Gradient Boosting',\"LightGBMRegressor\"]\n",
        "# Fit the models and visualize\n",
        "for name in models_to_plot:\n",
        "    model = regressors[name]\n",
        "    model.fit(X_train, y_train)\n",
        "    plot_permutation_importance(model, X_train, y_train, X_test, y_test, title=name, save_path=save_directory)\n",
        "\n"
      ],
      "metadata": {
        "id": "oM01qPfkFupt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To graph feature importances of which train set and test set located in one graph you can use the snippet below."
      ],
      "metadata": {
        "id": "rzFq1pKwJBGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # Importing pandas library for data manipulation and analysis\n",
        "import numpy as np  # Importing numpy library for numerical operations\n",
        "import warnings  # Importing warnings to manage warnings during the runtime\n",
        "import os  # Importing os for operating system dependent functionalities\n",
        "\n",
        "# Importing necessary classes and functions from sklearn for model building, preprocessing, and evaluation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "import matplotlib.pyplot as plt  # Importing matplotlib for plotting graphs\n",
        "\n",
        "# Loading the dataset\n",
        "data = pd.read_csv('path_to_your_data')\n",
        "\n",
        "# Transforming the target variable by applying a logarithmic function to make the distribution more symmetric\n",
        "data['Target_log'] = np.log(data['Target'] + 1)\n",
        "\n",
        "# Data preparation by dropping irrelevant columns\n",
        "X = data.drop(columns=['List_of_columns_to_drop'])\n",
        "y = data['Target_log']  # Specifying the target variable\n",
        "\n",
        "# Encoding categorical variables\n",
        "for col in X.select_dtypes(include='object').columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Best parameters\n",
        "rf_best_params = {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
        "xgb_best_params = {'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
        "bagging_best_params = {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 30}\n",
        "dt_best_params = {'criterion': 'friedman_mse', 'max_depth': 9, 'min_samples_leaf': 2, 'min_samples_split': 2, 'splitter': 'best'}\n",
        "\n",
        "# Create a dictionary of models\n",
        "regressors = {\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=None, min_samples_leaf=1, min_samples_split=2, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
        "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
        "    'Bagging': BaggingRegressor(n_estimators=30, max_features=1.0, max_samples=1.0, random_state=42),\n",
        "    'Decision Tree': DecisionTreeRegressor(criterion='friedman_mse', max_depth=9, min_samples_leaf=2, min_samples_split=2, splitter='best', random_state=42),\n",
        "    'Support Vector': SVR(),\n",
        "    'K-Neighbors': KNeighborsRegressor(),\n",
        "    'RidgeCV': RidgeCV(),\n",
        "    'Lasso': Lasso(),\n",
        "    'ElasticNet': ElasticNet(),\n",
        "    'SGD': SGDRegressor(),\n",
        "    'Gaussian Process': GaussianProcessRegressor(),\n",
        "    'BayesianRidge': BayesianRidge(),\n",
        "    'XGBoostRegressor': XGBRegressor(colsample_bytree=0.9, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8, random_state=42),\n",
        "    'LightGBMRegressor': LGBMRegressor(),\n",
        "    'SVR_poly': SVR(kernel='poly'),\n",
        "    'SVR_sigmoid': SVR(kernel='sigmoid'),\n",
        "    'SVR_rbf': SVR(kernel='rbf'),\n",
        "    'HuberRegressor': HuberRegressor(),\n",
        "    'RANSACRegressor': RANSACRegressor(),\n",
        "    'TheilSenRegressor': TheilSenRegressor(),\n",
        "    'KernelRidge': KernelRidge(),\n",
        "    'OMP': OrthogonalMatchingPursuit(),\n",
        "    'PoissonRegressor': PoissonRegressor(),\n",
        "    'TweedieRegressor': TweedieRegressor()\n",
        "}\n",
        "# Fit models and extract permutation importances\n",
        "perm_importances = {}  # Initializing the dictionary\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "def plot_overlapped_permutation_importance(model, X_train, y_train, X_test, y_test, title='', save_path=''):\n",
        "    # Compute permutation importance for training set\n",
        "    result_train = permutation_importance(model, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2)\n",
        "    sorted_idx_train = result_train.importances_mean.argsort()\n",
        "\n",
        "    # Compute permutation importance for test set\n",
        "    result_test = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2)\n",
        "    sorted_idx_test = result_test.importances_mean.argsort()\n",
        "\n",
        "    # Convert the results to DataFrame for better visualization\n",
        "    importances_train = pd.DataFrame(result_train.importances[sorted_idx_train].T, columns=X.columns[sorted_idx_train])\n",
        "    importances_test = pd.DataFrame(result_test.importances[sorted_idx_test].T, columns=X.columns[sorted_idx_test])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 12))\n",
        "\n",
        "    # Plotting for training set\n",
        "    importances_train.boxplot(ax=ax, vert=False, positions=np.arange(len(importances_train.columns))*2.0-0.4, widths=0.4, boxprops=dict(color='blue'), medianprops=dict(color='blue'), whiskerprops=dict(color='blue'), capprops=dict(color='blue'))\n",
        "\n",
        "    # Plotting for test set\n",
        "    importances_test.boxplot(ax=ax, vert=False, positions=np.arange(len(importances_test.columns))*2.0+0.4, widths=0.4, boxprops=dict(color='red'), medianprops=dict(color='red'), whiskerprops=dict(color='red'), capprops=dict(color='red'))\n",
        "\n",
        "    # Tweaking the plot appearance\n",
        "    ax.set_yticks(np.arange(len(importances_train.columns))*2.0)\n",
        "    ax.set_yticklabels(importances_train.columns)\n",
        "    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
        "    ax.set_xlabel(\"Decrease in accuracy score\")\n",
        "    ax.set_title(f\"Overlapped Permutation Importances - {title}\")\n",
        "    ax.grid(False)\n",
        "    # Custom legend\n",
        "    custom_lines = [Line2D([0], [0], color=\"blue\", lw=4), Line2D([0], [0], color=\"red\", lw=4)]\n",
        "    ax.legend(custom_lines, ['Train', 'Test'], loc=\"lower right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if save_path:\n",
        "        png_path = os.path.join(save_path, f\"{title}_Overlapped_Permutation_Importance3.png\")\n",
        "        tiff_path = os.path.join(save_path, f\"{title}_Overlapped_Permutation_Importance3.tiff\")\n",
        "\n",
        "        fig.savefig(png_path, dpi=1000)\n",
        "        fig.savefig(tiff_path, dpi=1000)\n",
        "\n",
        "    plt.close(fig)  # close the figure\n",
        "# Create directories if they don't exist\n",
        "save_directory = \"path_to_save_directory\"\n",
        "if not os.path.exists(save_directory):\n",
        "    os.makedirs(save_directory)\n",
        "    # Models to visualize\n",
        "models_to_plot = ['XGBoostRegressor', 'Random Forest', 'Bagging', 'Gradient Boosting',\"LightGBMRegressor\"]\n",
        "# Fit the models and visualize\n",
        "for name in models_to_plot:\n",
        "    model = regressors[name]\n",
        "    model.fit(X_train, y_train)\n",
        "    plot_permutation_importance(model, X_train, y_train, X_test, y_test, title=name, save_path=save_directory)"
      ],
      "metadata": {
        "id": "T0gYf31zIkwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You can use the code below to get hyperparameters of all models that is used for the regression.In addition to test and train R^2 values you can obtain other regression metrics such as MAE, MSE, RMSE, MAPE with the code below."
      ],
      "metadata": {
        "id": "6lTeV_kZLPFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.linear_model import (HuberRegressor, RANSACRegressor, TheilSenRegressor,\n",
        "                                  OrthogonalMatchingPursuit, PoissonRegressor,\n",
        "                                  TweedieRegressor, RidgeCV, Lasso,\n",
        "                                  ElasticNet, SGDRegressor, BayesianRidge)\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,\n",
        "                              AdaBoostRegressor, BaggingRegressor, StackingRegressor)\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from pygam import LinearGAM, s, f\n",
        "\n",
        "# Define a function to calculate Mean Absolute Percentage Error\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-6))) * 100\n",
        "\n",
        "# Loading the dataset\n",
        "data = pd.read_csv('path_to_your_data')\n",
        "\n",
        "# Transforming the target variable by applying a logarithmic function to make the distribution more symmetric\n",
        "data['Target_log'] = np.log(data['Target'] + 1)\n",
        "\n",
        "# Data preparation by dropping irrelevant columns\n",
        "X = data.drop(columns=['List_of_columns_to_drop'])\n",
        "y = data['Target_log']  # Specifying the target variable\n",
        "\n",
        "\n",
        "# Encode categorical features\n",
        "for col in X.select_dtypes(include='object').columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define a dictionary with models\n",
        "regressors = {\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=None, min_samples_leaf=1, min_samples_split=2, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
        "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
        "    'Bagging': BaggingRegressor(n_estimators=30, max_features=1.0, max_samples=1.0, random_state=42),\n",
        "    'Decision Tree': DecisionTreeRegressor(criterion='friedman_mse', max_depth=9, min_samples_leaf=2, min_samples_split=2, splitter='best', random_state=42),\n",
        "    'Support Vector': SVR(),\n",
        "    'K-Neighbors': KNeighborsRegressor(),\n",
        "    'RidgeCV': RidgeCV(),\n",
        "    'Lasso': Lasso(),\n",
        "    'ElasticNet': ElasticNet(),\n",
        "    'SGD': SGDRegressor(),\n",
        "    'Gaussian Process': GaussianProcessRegressor(),\n",
        "    'BayesianRidge': BayesianRidge(),\n",
        "    'XGBoostRegressor': XGBRegressor(colsample_bytree=0.9, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8, random_state=42),\n",
        "    'LightGBMRegressor': LGBMRegressor(),\n",
        "    'SVR_poly': SVR(kernel='poly'),\n",
        "    'SVR_sigmoid': SVR(kernel='sigmoid'),\n",
        "    'SVR_rbf': SVR(kernel='rbf'),\n",
        "    'HuberRegressor': HuberRegressor(),\n",
        "    'SVR_linear': SVR(kernel='linear'),\n",
        "    'RANSACRegressor': RANSACRegressor(),\n",
        "    'TheilSenRegressor': TheilSenRegressor(),\n",
        "    'KernelRidge': KernelRidge(),\n",
        "    'OMP': OrthogonalMatchingPursuit(),\n",
        "    'PoissonRegressor': PoissonRegressor(),\n",
        "    'TweedieRegressor': TweedieRegressor()\n",
        "}\n",
        "\n",
        "# Initialize lists to store results\n",
        "results = []\n",
        "stacked_results = []\n",
        "\n",
        "# Fit models and evaluate performance\n",
        "# Extend the script to save hyperparameters of each model\n",
        "for model_name, model in regressors.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Extract hyperparameters of the model\n",
        "    hyperparameters = model.get_params()\n",
        "\n",
        "    # Calculate metrics and store them along with hyperparameters in the results list\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Hyperparameters': hyperparameters,  # Save hyperparameters as a nested dictionary\n",
        "        'Train R2 Score': r2_score(y_train, y_train_pred),\n",
        "        'Test R2 Score': r2_score(y_test, y_test_pred),\n",
        "        'Train MSE': mean_squared_error(y_train, y_train_pred),\n",
        "        'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
        "        'Train MAE': mean_absolute_error(y_train, y_train_pred),\n",
        "        'Train MAPE': mean_absolute_percentage_error(y_train, y_train_pred),\n",
        "        'Test MSE': mean_squared_error(y_test, y_test_pred),\n",
        "        'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
        "        'Test MAE': mean_absolute_error(y_test, y_test_pred),\n",
        "        'Test MAPE': mean_absolute_percentage_error(y_test, y_test_pred)\n",
        "    })\n",
        "# Define and fit a GAM model\n",
        "gam = LinearGAM(s(0) + s(1)).fit(X_train, y_train)\n",
        "gam_train_pred = gam.predict(X_train)\n",
        "gam_test_pred = gam.predict(X_test)\n",
        "\n",
        "# Also, save the hyperparameters of the GAM model\n",
        "gam_hyperparameters = {'terms': str(gam.terms)}  # Extract relevant hyperparameters or configurations\n",
        "\n",
        "results.append({\n",
        "    'Model': 'GAM',\n",
        "    'Hyperparameters': gam_hyperparameters,\n",
        "    'Train R2 Score': r2_score(y_train, gam_train_pred),\n",
        "    'Test R2 Score': r2_score(y_test, gam_test_pred),\n",
        "    'Train MSE': mean_squared_error(y_train, gam_train_pred),\n",
        "    'Train RMSE': np.sqrt(mean_squared_error(y_train, gam_train_pred)),\n",
        "    'Train MAE': mean_absolute_error(y_train, gam_train_pred),\n",
        "    'Train MAPE': mean_absolute_percentage_error(y_train, gam_train_pred),\n",
        "    'Test MSE': mean_squared_error(y_test, gam_test_pred),\n",
        "    'Test RMSE': np.sqrt(mean_squared_error(y_test, gam_test_pred)),\n",
        "    'Test MAE': mean_absolute_error(y_test, gam_test_pred),\n",
        "    'Test MAPE': mean_absolute_percentage_error(y_test, gam_test_pred)\n",
        "})\n",
        "\n",
        "# For stacked models, you can save the names of the models being stacked as hyperparameters\n",
        "for stacked_model in stacked_results:\n",
        "    model_names = [estimator[0] for estimator in stacking_regressor.estimators_]\n",
        "    stacked_model['Hyperparameters'] = {'Models': model_names}\n",
        "\n",
        "# Save results to DataFrames and then to CSV or Excel\n",
        "results_df = pd.DataFrame(results)\n",
        "stacked_results_df = pd.DataFrame(stacked_results)\n",
        "results_df.to_csv('path_to_save_your_results/results_with_hyperparameters.csv', index=False)\n",
        "stacked_results_df.to_csv('path_to_save_your_results_of_stacked_models/stacked_results_with_hyperparameters.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "skDgYdCIKtzL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}