{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The snippet below consists back propagation neural networ with sigmoid activation function with 2-3 neurons and linear output activation function. You can also use this code the compare different activation functions namely 'relu', 'tanh', 'sigmoid', 'linear', 'elu', 'leaky_relu' and graph the best results of each run with defined regression metrics."
      ],
      "metadata": {
        "id": "LqBl6FrBUpIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, Callback\n"
      ],
      "metadata": {
        "id": "9uZ9Pl1VdehM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J2HTQ5QUmov"
      },
      "outputs": [],
      "source": [
        "print(\"...............Reading the Dataset and Dataset Pre-Processing ................\")\n",
        "start_time = time.time()\n",
        "# Adjust your path accordingly\n",
        "# Loading the dataset\n",
        "data = pd.read_csv('path_to_your_data')\n",
        "\n",
        "# Transforming the target variable by applying a logarithmic function to make the distribution more symmetric\n",
        "data['Target_log'] = np.log(data['Target'] + 1)\n",
        "\n",
        "# Data preparation by dropping irrelevant columns\n",
        "X = data.drop(columns=['List_of_columns_to_drop'])\n",
        "y = data['Target_log']  # Specifying the target variable\n",
        "\n",
        "# Encoding categorical variables\n",
        "for col in X.select_dtypes(include='object').columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "y_train = y_train.values.reshape(-1, 1)\n",
        "y_test = y_test.values.reshape(-1, 1)\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "print(\"Time Cost for Pre-processing and Reading the Dataset: %f seconds \\n \" % total_time)\n",
        "\n",
        "def build_bpnn_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation=activation_function, input_dim=input_dim))  # Input layer\n",
        "    model.add(Dense(3, activation='sigmoid'))  # Hidden layer with 2-3 neurons and sigmoid activation\n",
        "    model.add(Dense(1, activation='linear'))  # Output layer with linear activation\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
        "    return model\n",
        "class R2Callback(Callback):\n",
        "    def __init__(self, train_data, validation_data):\n",
        "        super(R2Callback, self).__init__()\n",
        "        self.train_data = train_data\n",
        "        self.validation_data = validation_data\n",
        "        self.train_r2s = []\n",
        "        self.validation_r2s = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_train_pred = self.model.predict(self.train_data[0])\n",
        "        train_r2 = r2_score(self.train_data[1], y_train_pred)\n",
        "        self.train_r2s.append(train_r2)\n",
        "        y_val_pred = self.model.predict(self.validation_data[0])\n",
        "        val_r2 = r2_score(self.validation_data[1], y_val_pred)\n",
        "        self.validation_r2s.append(val_r2)\n",
        "        print(f\" - train_r2: {train_r2:.4f} - val_r2: {val_r2:.4f}\")\n",
        "\n",
        "activation_functions = ['relu', 'tanh', 'sigmoid', 'linear', 'elu', 'leaky_relu']\n",
        "histories = {}\n",
        "r2_callbacks = {}\n",
        "results = []\n",
        "\n",
        "for activation_function in activation_functions:\n",
        "    print(f\"Training BPNN model with {activation_function} activation function\")\n",
        "    ann_model = build_ann_model(x_train.shape[1], activation_function)\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
        "    r2_callback = R2Callback(train_data=(x_train, y_train), validation_data=(x_test, y_test))\n",
        "    history = ann_model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test), callbacks=[early_stop, r2_callback], verbose=1)\n",
        "    histories[activation_function] = history\n",
        "    r2_callbacks[activation_function] = r2_callback\n",
        "    results.append({\n",
        "        'Model': f'BPNN_{activation_function}',\n",
        "        'Final Train Loss': history.history['loss'][-1],\n",
        "        'Final Validation Loss': history.history['val_loss'][-1],\n",
        "        'Final Train MAE': history.history['mae'][-1],\n",
        "        'Final Validation MAE': history.history['val_mae'][-1],\n",
        "        'Final Train R²': r2_callback.train_r2s[-1],\n",
        "        'Final Validation R²': r2_callback.validation_r2s[-1]\n",
        "    })\n",
        "\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "for i, activation_function in enumerate(activation_functions):\n",
        "    plt.subplot(3, len(activation_functions), i + 1)\n",
        "    plt.plot(histories[activation_function].history['loss'], label='Train Loss')\n",
        "    plt.plot(histories[activation_function].history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{activation_function} Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(3, len(activation_functions), i + 1 + len(activation_functions))\n",
        "    plt.plot(histories[activation_function].history['mae'], label='Train MAE')\n",
        "    plt.plot(histories[activation_function].history['val_mae'], label='Validation MAE')\n",
        "    plt.title(f'{activation_function} MAE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(3, len(activation_functions), i + 1 + 2 * len(activation_functions))\n",
        "    plt.plot(r2_callbacks[activation_function].train_r2s, label='Train R²')\n",
        "    plt.plot(r2_callbacks[activation_function].validation_r2s, label='Validation R²')\n",
        "    plt.title(f'{activation_function} R²')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('R²')\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"Tabulated Results: \")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instead of running a BPNN algorithm in which some layer informations already defined, you can run the below ANN code to search the best hperparameters for each activation functions. If your local machine is struggleing to run the code because the intense computation needed, use the example code which is revised for the tanh activation algorithm. You can use that code for other activation functions too by changing the name of the activaiton functions."
      ],
      "metadata": {
        "id": "OJxsJxp4UniG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class R2Callback(Callback):\n",
        "    def __init__(self, train_data, validation_data):\n",
        "        super(R2Callback, self).__init__()\n",
        "        self.train_data = train_data\n",
        "        self.validation_data = validation_data\n",
        "        self.train_r2s = []\n",
        "        self.validation_r2s = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        X_train, y_train = self.train_data\n",
        "        X_val, y_val = self.validation_data\n",
        "        y_train_pred = self.model.predict(X_train)\n",
        "        y_val_pred = self.model.predict(X_val)\n",
        "        train_r2 = r2_score(y_train, y_train_pred)\n",
        "        val_r2 = r2_score(y_val, y_val_pred)\n",
        "        self.train_r2s.append(train_r2)\n",
        "        self.validation_r2s.append(val_r2)\n",
        "\n",
        "print(\"...............Reading the Dataset and Dataset Pre-Processing ................\")\n",
        "start_time = time.time()\n",
        "# Adjust your path accordingly\n",
        "# Loading the dataset\n",
        "data = pd.read_csv('path_to_your_data')\n",
        "\n",
        "# Transforming the target variable by applying a logarithmic function to make the distribution more symmetric\n",
        "data['Target_log'] = np.log(data['Target'] + 1)\n",
        "\n",
        "# Data preparation by dropping irrelevant columns\n",
        "X = data.drop(columns=['List_of_columns_to_drop'])\n",
        "y = data['Target_log']  # Specifying the target variable\n",
        "\n",
        "# Encoding categorical variables\n",
        "for col in X.select_dtypes(include='object').columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "def build_model(input_dim, num_hidden_layers, activation='relu', is_leaky_relu=False):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim))\n",
        "    if is_leaky_relu:\n",
        "        model.add(LeakyReLU(alpha=0.01))\n",
        "    else:\n",
        "        model.add(Activation(activation))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    for _ in range(num_hidden_layers - 1):\n",
        "        model.add(Dense(64))\n",
        "        if is_leaky_relu:\n",
        "            model.add(LeakyReLU(alpha=0.01))\n",
        "        else:\n",
        "            model.add(Activation(activation))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(32, activation=activation))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "activation_functions = ['relu', 'tanh', 'sigmoid', 'linear', 'elu', 'leaky_relu']\n",
        "num_hidden_layers_list = [1, 2, 3, 4, 5]\n",
        "additional_learning_rates = [0.01, 0.0001]\n",
        "additional_batch_sizes = [32, 128]\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "best_val_r2 = float('-inf')\n",
        "results = []\n",
        "\n",
        "for activation_function in activation_functions:\n",
        "    for num_hidden_layers in num_hidden_layers_list:\n",
        "        for batch_size in [32, 64] + additional_batch_sizes:\n",
        "            for learning_rate in [0.001] + additional_learning_rates:\n",
        "                print(f\"Training model with activation_function={activation_function}, num_hidden_layers={num_hidden_layers}, batch_size={batch_size}, learning_rate={learning_rate}\")\n",
        "\n",
        "                r2_callback = R2Callback(train_data=(X_train, y_train), validation_data=(X_test, y_test))\n",
        "                lr_schedule_callback = LearningRateScheduler(lambda epoch, lr: lr if epoch < 10 else lr * 0.9)\n",
        "\n",
        "                is_leaky_relu = activation_function == 'leaky_relu'\n",
        "                ann_model = build_model(X_train.shape[1], num_hidden_layers, activation_function, is_leaky_relu)\n",
        "                ann_model.optimizer.lr.assign(learning_rate)\n",
        "\n",
        "                history = ann_model.fit(X_train, y_train, epochs=100, batch_size=batch_size,\n",
        "                        validation_data=(X_test, y_test), verbose=1,\n",
        "                        callbacks=[r2_callback, lr_schedule_callback, early_stop])\n",
        "\n",
        "                final_train_r2 = r2_callback.train_r2s[-1] if r2_callback.train_r2s else float('-inf')\n",
        "                final_val_r2 = r2_callback.validation_r2s[-1] if r2_callback.validation_r2s else float('-inf')\n",
        "\n",
        "                results.append({\n",
        "                    'Activation Function': activation_function,\n",
        "                    'Number of Hidden Layers': num_hidden_layers,\n",
        "                    'Batch Size': batch_size,\n",
        "                    'Learning Rate': learning_rate,\n",
        "                    'Final Train Loss': history.history['loss'][-1] if 'loss' in history.history else float('nan'),\n",
        "                    'Final Validation Loss': history.history['val_loss'][-1] if 'val_loss' in history.history else float('nan'),\n",
        "                    'Final Train MAE': history.history['mae'][-1] if 'mae' in history.history else float('nan'),\n",
        "                    'Final Validation MAE': history.history['val_mae'][-1] if 'val_mae' in history.history else float('nan'),\n",
        "                    'Final Train R²': final_train_r2,\n",
        "                    'Final Validation R²': final_val_r2\n",
        "                })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "# Plot the best model\n",
        "if best_model:\n",
        "    activation_function, num_hidden_layers, history, r2_callback = best_model\n",
        "    metrics = ['loss', 'mae', 'R²']\n",
        "    num_metrics = len(metrics)\n",
        "    fig, axs = plt.subplots(1, num_metrics, figsize=(5 * num_metrics, 5))\n",
        "\n",
        "    train_metrics = [history.history['loss'], history.history['mae'], history.history['train_r2']]\n",
        "    val_metrics = [history.history['val_loss'], history.history['val_mae'], history.history['val_r2']]\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        axs[i].plot(train_metrics[i], label=f'Train {metric}')\n",
        "        axs[i].plot(val_metrics[i], label=f'Validation {metric}')\n",
        "        axs[i].set_title(f'{activation_function}_{num_hidden_layers}hidden {metric}')\n",
        "        axs[i].set_xlabel('Epoch')\n",
        "        axs[i].set_ylabel(metric)\n",
        "        axs[i].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # Saving the plot in PNG format with 1000 dpi\n",
        "    plt.savefig(f\"{save_directory}best_model_plot.png\", dpi=1000)\n",
        "    # Saving the plot in TIFF format with 1000 dpi\n",
        "    plt.savefig(f\"{save_directory}best_model_plot.tiff\", dpi=1000)\n",
        "    plt.show()\n",
        "\n",
        "save_directory\n"
      ],
      "metadata": {
        "id": "OyqBXDn3ZSc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Individual Activation Functions, change the activation function which is \"sigmoid\" here to the name of the activation function you want to investigate."
      ],
      "metadata": {
        "id": "P_MTynMEfLZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class R2Callback(Callback):\n",
        "    def __init__(self, train_data, validation_data):\n",
        "        super(R2Callback, self).__init__()\n",
        "        self.train_data = train_data\n",
        "        self.validation_data = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        X_train, y_train = self.train_data\n",
        "        X_val, y_val = self.validation_data\n",
        "        y_train_pred = self.model.predict(X_train)\n",
        "        y_val_pred = self.model.predict(X_val)\n",
        "        train_r2 = r2_score(y_train, y_train_pred)\n",
        "        val_r2 = r2_score(y_val, y_val_pred)\n",
        "        logs['train_r2'] = train_r2\n",
        "        logs['val_r2'] = val_r2\n",
        "\n",
        "\n",
        "print(\"...............Reading the Dataset and Dataset Pre-Processing ................\")\n",
        "start_time = time.time()\n",
        "# Adjust your path accordingly\n",
        "# Loading the dataset\n",
        "data = pd.read_csv('path_to_your_data')\n",
        "\n",
        "# Transforming the target variable by applying a logarithmic function to make the distribution more symmetric\n",
        "data['Target_log'] = np.log(data['Target'] + 1)\n",
        "\n",
        "# Data preparation by dropping irrelevant columns\n",
        "X = data.drop(columns=['List_of_columns_to_drop'])\n",
        "y = data['Target_log']  # Specifying the target variable\n",
        "\n",
        "# Encoding categorical variables\n",
        "for col in X.select_dtypes(include='object').columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "def build_model(input_dim, num_hidden_layers, activation='sigmoid'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim))\n",
        "\n",
        "    if activation == 'leaky_relu':\n",
        "        model.add(LeakyReLU(alpha=0.01))\n",
        "    else:\n",
        "        model.add(Activation(activation))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    for _ in range(num_hidden_layers - 1):\n",
        "        model.add(Dense(64))\n",
        "\n",
        "        if activation == 'leaky_relu':\n",
        "            model.add(LeakyReLU(alpha=0.01))\n",
        "        else:\n",
        "            model.add(Activation(activation))\n",
        "\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(32, activation=activation))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "\n",
        "activation_functions = ['sigmoid']\n",
        "num_hidden_layers_list = [1, 2, 3, 4, 5]\n",
        "additional_batch_sizes = [32, 128]\n",
        "additional_learning_rates = [0.01, 0.0001]\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "best_val_r2 = float('-inf')\n",
        "best_model = None\n",
        "results = []\n",
        "\n",
        "for activation_function in activation_functions:\n",
        "    for num_hidden_layers in num_hidden_layers_list:\n",
        "        for batch_size in [32, 64] + additional_batch_sizes:\n",
        "            for learning_rate in [0.001] + additional_learning_rates:\n",
        "                print(f\"Training model with activation_function={activation_function}, num_hidden_layers={num_hidden_layers}, batch_size={batch_size}, learning_rate={learning_rate}\")\n",
        "\n",
        "                r2_callback = R2Callback(train_data=(X_train, y_train), validation_data=(X_test, y_test))\n",
        "                lr_schedule_callback = LearningRateScheduler(lambda epoch, lr: lr if epoch < 10 else lr * 0.9)\n",
        "\n",
        "                ann_model = build_model(X_train.shape[1], num_hidden_layers, activation_function)\n",
        "                ann_model.optimizer.lr.assign(learning_rate)\n",
        "\n",
        "                history = ann_model.fit(X_train, y_train, epochs=100, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1,\n",
        "                                        callbacks=[r2_callback, lr_schedule_callback, early_stop])\n",
        "                # Print metrics after the first training iteration to verify the results.\n",
        "                print(\"Train Loss:\", history.history['loss'][0])\n",
        "                print(\"Validation Loss:\", history.history['val_loss'][0])\n",
        "                print(\"Train MAE:\", history.history['mae'][0])\n",
        "                print(\"Validation MAE:\", history.history['val_mae'][0])\n",
        "                print(\"Train R²:\", history.history['train_r2'][0])\n",
        "                print(\"Validation R²:\", history.history['val_r2'][0])\n",
        "                final_val_r2 = history.history['val_r2'][-1] if 'val_r2' in history.history else float('-inf')\n",
        "\n",
        "                if final_val_r2 > best_val_r2:\n",
        "                    best_val_r2 = final_val_r2\n",
        "                    best_model = (activation_function, num_hidden_layers, history, r2_callback)\n",
        "\n",
        "                results.append({\n",
        "                    'Activation Function': activation_function,\n",
        "                    'Number of Hidden Layers': num_hidden_layers,\n",
        "                    'Batch Size': batch_size,\n",
        "                    'Learning Rate': learning_rate,\n",
        "                    'Final Train Loss': history.history['loss'][-1] if 'loss' in history.history else float('nan'),\n",
        "                    'Final Validation Loss': history.history['val_loss'][-1] if 'val_loss' in history.history else float('nan'),\n",
        "                    'Final Train MAE': history.history['mae'][-1] if 'mae' in history.history else float('nan'),\n",
        "                    'Final Validation MAE': history.history['val_mae'][-1] if 'val_mae' in history.history else float('nan'),\n",
        "                    'Final Train R²': history.history['train_r2'][-1] if 'train_r2' in history.history else float('nan'),\n",
        "                    'Final Validation R²': final_val_r2\n",
        "                })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n",
        "\n",
        "# Define a directory to save the plots\n",
        "save_directory = \"/saving_directory_for_plots/\"\n",
        "\n",
        "\n",
        "\n",
        "# Plot the best model\n",
        "if best_model:\n",
        "    activation_function, num_hidden_layers, history, r2_callback = best_model\n",
        "    metrics = ['loss', 'mae', 'R²']\n",
        "    num_metrics = len(metrics)\n",
        "    fig, axs = plt.subplots(1, num_metrics, figsize=(5 * num_metrics, 5))\n",
        "\n",
        "    train_metrics = [history.history['loss'], history.history['mae'], history.history['train_r2']]\n",
        "    val_metrics = [history.history['val_loss'], history.history['val_mae'], history.history['val_r2']]\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        axs[i].plot(train_metrics[i], label=f'Train {metric}')\n",
        "        axs[i].plot(val_metrics[i], label=f'Validation {metric}')\n",
        "        axs[i].set_title(f'{activation_function}_{num_hidden_layers}hidden {metric}')\n",
        "        axs[i].set_xlabel('Epoch')\n",
        "        axs[i].set_ylabel(metric)\n",
        "        axs[i].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # Saving the plot in PNG format with 1000 dpi\n",
        "    plt.savefig(f\"{save_directory}best_model_plot.png\", dpi=1000)\n",
        "    # Saving the plot in TIFF format with 1000 dpi\n",
        "    plt.savefig(f\"{save_directory}best_model_plot.tiff\", dpi=1000)\n",
        "    plt.show()\n",
        "\n",
        "save_directory\n"
      ],
      "metadata": {
        "id": "ar1l3XuBfKeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running each code, you can save the results_df dataframe to csv file to check the results. You can also save the plots for the best results too."
      ],
      "metadata": {
        "id": "4QbpeXCyfvQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv('path_to_saving_directory/file_name.csv', index=False)\n"
      ],
      "metadata": {
        "id": "TilFB0Ugfuoi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}