{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUCbBL0J1eVn"
      },
      "outputs": [],
      "source": [
        "!pip install catboost\n",
        "!pip install pygam\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.linear_model import (HuberRegressor, RANSACRegressor, TheilSenRegressor,\n",
        "                                  OrthogonalMatchingPursuit, PoissonRegressor,\n",
        "                                  TweedieRegressor, RidgeCV, Lasso,\n",
        "                                  ElasticNet, SGDRegressor, BayesianRidge)\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,\n",
        "                              AdaBoostRegressor, BaggingRegressor, StackingRegressor)\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from pygam import LinearGAM, s, f\n",
        "\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('path_to_your_file/file_name.csv')\n",
        "\n",
        "# Log-transform the target variable\n",
        "data['Cellular_Uptake_pg_Au_cell_log'] = np.log(data['Cellular_Uptake_pg_Au_cell'] + 1)\n",
        "\n",
        "# Data preparation, you can change the column names to adjust your dataset\n",
        "always_drop_columns = ['Row_number', 'row_number', 'Cell_source_system', 'Particle_ID', 'Coating_type', 'Coating_category', 'Coating category new','NP_mass_pg', 'Reference_DOI', 'Cellular_Uptake_pg_Au_cell_log', 'Cellular_uptake_number_of_NP', 'Cellular_Uptake_pg_Au_cell']\n",
        "X = data.drop(columns=always_drop_columns + ['Coating_type', 'Coating_category', 'Coating category new'])\n",
        "y = data['Cellular_Uptake_pg_Au_cell_log']\n",
        "\n",
        "# Encoding categorical variables\n",
        "for col in X.select_dtypes(include='object').columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Hyperparameters for tuning, use this part to ensure which hyperparameters are better for that individual regressor\n",
        "\"\"\"rf_params = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "xgb_params = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.8, 0.9, 1],\n",
        "    'colsample_bytree': [0.8, 0.9, 1]\n",
        "}\n",
        "\n",
        "bagging_params = {\n",
        "    'n_estimators': [10, 20, 30],\n",
        "    'max_samples': [0.5, 0.8, 1.0],\n",
        "    'max_features': [0.5, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "dt_params = {\n",
        "    'criterion': ['mse', 'friedman_mse', 'mae'],\n",
        "    'splitter': ['best', 'random'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# GridSearchCV for each model\n",
        "rf_grid = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=5, n_jobs=-1)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "print(\"Best parameters for Random Forest:\", rf_grid.best_params_)\n",
        "\n",
        "xgb_grid = GridSearchCV(XGBRegressor(random_state=42), xgb_params, cv=5, n_jobs=-1)\n",
        "xgb_grid.fit(X_train, y_train)\n",
        "print(\"Best parameters for XGBoost:\", xgb_grid.best_params_)\n",
        "\n",
        "bagging_grid = GridSearchCV(BaggingRegressor(random_state=42), bagging_params, cv=5, n_jobs=-1)\n",
        "bagging_grid.fit(X_train, y_train)\n",
        "print(\"Best parameters for Bagging:\", bagging_grid.best_params_)\n",
        "\n",
        "dt_grid = GridSearchCV(DecisionTreeRegressor(random_state=42), dt_params, cv=5, n_jobs=-1)\n",
        "dt_grid.fit(X_train, y_train)\n",
        "print(\"Best parameters for Decision Tree:\", dt_grid.best_params_)\"\"\"\n",
        "\n",
        "\n",
        "# Define and train various regressors with the training data, the hyperparameters in this code has been determined by running the codes between the \"\"\"grid search\"\"\" statements\n",
        "\n",
        "regressors = {\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=None, min_samples_leaf=1, min_samples_split=2, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
        "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
        "    'Bagging': BaggingRegressor(n_estimators=30, max_features=1.0, max_samples=1.0, random_state=42),\n",
        "    'Decision Tree': DecisionTreeRegressor(criterion='friedman_mse', max_depth=9, min_samples_leaf=2, min_samples_split=2, splitter='best', random_state=42),\n",
        "    'Support Vector': SVR(),\n",
        "    'K-Neighbors': KNeighborsRegressor(),\n",
        "    'RidgeCV': RidgeCV(),\n",
        "    'Lasso': Lasso(),\n",
        "    'ElasticNet': ElasticNet(),\n",
        "    'SGD': SGDRegressor(),\n",
        "    'Gaussian Process': GaussianProcessRegressor(),\n",
        "    'BayesianRidge': BayesianRidge(),\n",
        "    'XGBoostRegressor': XGBRegressor(colsample_bytree=0.9, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8, random_state=42),\n",
        "    'LightGBMRegressor': LGBMRegressor(),\n",
        "    'SVR_poly': SVR(kernel='poly'),\n",
        "    'SVR_sigmoid': SVR(kernel='sigmoid'),\n",
        "    'SVR_rbf': SVR(kernel='rbf'),\n",
        "    'HuberRegressor': HuberRegressor(),\n",
        "    'RANSACRegressor': RANSACRegressor(),\n",
        "    'TheilSenRegressor': TheilSenRegressor(),\n",
        "    'KernelRidge': KernelRidge(),\n",
        "    'OMP': OrthogonalMatchingPursuit(),\n",
        "    'PoissonRegressor': PoissonRegressor(),\n",
        "    'TweedieRegressor': TweedieRegressor()\n",
        "}\n",
        "results_log_transformed = []\n",
        "for model_name, model in regressors.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    r2_train = r2_score(y_train, y_train_pred)\n",
        "    r2_test = r2_score(y_test, y_test_pred)\n",
        "    results_log_transformed.append({\n",
        "        'Model': model_name,\n",
        "        'Train R2 Score': r2_train,\n",
        "        'Test R2 Score': r2_test\n",
        "    })\n",
        "\n",
        "# Training and evaluating GAM separately\n",
        "gam = LinearGAM(s(0) + s(1) + f(2)).fit(X_train, y_train)\n",
        "gam_train_pred = gam.predict(X_train)\n",
        "gam_test_pred = gam.predict(X_test)\n",
        "results_log_transformed.append({\n",
        "    'Model': 'GAM',\n",
        "    'Train R2 Score': r2_score(y_train, gam_train_pred),\n",
        "    'Test R2 Score': r2_score(y_test, gam_test_pred)\n",
        "})\n",
        "# Perform stacking using top models and append results to results_log_transformed,\n",
        "\n",
        "stacked_models = []\n",
        "\n",
        "for i in [2, 3, 4]:\n",
        "    # Filter out already stacked models\n",
        "    top_models = sorted([r for r in results_log_transformed if r['Model'] not in stacked_models],\n",
        "                        key=lambda x: x['Test R2 Score'], reverse=True)[:i]\n",
        "\n",
        "    estimators = [(model['Model'], regressors[model['Model']]) for model in top_models]\n",
        "\n",
        "    stacking_regressor = StackingRegressor(estimators=estimators, cv=5)\n",
        "    stacking_regressor.fit(X_train, y_train)\n",
        "    stacked_train_pred = stacking_regressor.predict(X_train)\n",
        "    stacked_test_pred = stacking_regressor.predict(X_test)\n",
        "\n",
        "    stacked_r2_train = r2_score(y_train, stacked_train_pred)\n",
        "    stacked_r2_test = r2_score(y_test, stacked_test_pred)\n",
        "\n",
        "    stacked_model_name = f'StackingRegressor_Top{i}'\n",
        "    results_log_transformed.append({\n",
        "        'Model': stacked_model_name,\n",
        "        'Train R2 Score': stacked_r2_train,\n",
        "        'Test R2 Score': stacked_r2_test\n",
        "    })\n",
        "    stacked_models.append(stacked_model_name)\n",
        "\n",
        "results_df = pd.DataFrame(results_log_transformed).sort_values(by='Test R2 Score', ascending=False)\n",
        "print(results_df)\n",
        "\n",
        "#by running this code snippet it is achiavable to get test and train R^2's of both individual and stacked models.\n"
      ],
      "metadata": {
        "id": "49Y7ywMS1vxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the R^2 values of the stacked models did not improved much, only individual regressors had been chosen to plot feature importances. The selection of algorithms for visualization was made by manually sorting the r^2 values. If you want to do this through code, you can modify your snippet with the line below.\n",
        "\n",
        "**IMPORTANT NOTE:**  \n",
        "Not every algorithm may contain the feature importance attribute."
      ],
      "metadata": {
        "id": "AhUpRyTQ9II4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the names of the top-performing models\n",
        "top_models = sorted(results_log_transformed, key=lambda x: x['Test R2 Score'], reverse=True)[:4]  # Top 4 models\n",
        "top_model_names = [model['Model'] for model in top_models if model['Model'] in regressors and hasattr(regressors[model['Model']], 'feature_importances_')]\n",
        "\n",
        "# Define positions of bars for each model\n",
        "positions = np.array(range(len(sorted_idx)))\n",
        "\n",
        "# Plotting bars\n",
        "for idx, name in enumerate(top_model_names):\n",
        "    values = importances[name]\n",
        "    plt.barh(positions + width*idx, values[sorted_idx], color=colors[idx], label=name, height=width)\n"
      ],
      "metadata": {
        "id": "TnIRvJwZA1-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# FEATURE IMPORTANCE VISUALIZATION**"
      ],
      "metadata": {
        "id": "x_JGk-Xq_PvW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dhk0u2lf_Pd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# Define the models you want to plot according to R^2's you got from running the code above\n",
        "models_to_plot = ['XGBoostRegressor', 'Random Forest', 'Bagging', 'Gradient Boosting', 'LightGBMRegressor']\n",
        "\n",
        "# Colors for the models\n",
        "colors = ['#5081bc', '#7298bc', '#fe70bc', '#fab8dc', '#76b4a8']\n",
        "\n",
        "# Normalize feature importances, the scale of the feature importances for LightGBMRegressor is different from others so normalizing would be a good option\n",
        "for name, importance_values in importances.items():\n",
        "    importances[name] = importance_values / np.sum(importance_values)\n",
        "\n",
        "# Define positions of bars for each model\n",
        "positions = np.array(range(len(sorted_idx)))\n",
        "\n",
        "# Calculate width of a bar\n",
        "width = 0.15\n",
        "\n",
        "# Adjust the positions array to ensure bars are side-by-side\n",
        "positions = positions - (width * len(models_to_plot) / 2)\n",
        "\n",
        "# Plotting bars\n",
        "for idx, name in enumerate(models_to_plot):\n",
        "    values = importances[name]\n",
        "    plt.barh(positions + width*idx, values[sorted_idx], color=colors[idx], label=name, height=width, align='center')\n",
        "\n",
        "# Updating y-ticks to be in the center of grouped bars\n",
        "plt.yticks(positions + width*(len(models_to_plot) - 1)/2, X.columns[sorted_idx])\n",
        "\n",
        "plt.xlabel('Importance', fontsize=14)\n",
        "plt.ylabel('Features', fontsize=14)\n",
        "plt.title('Feature Importances\\n', fontsize=16)\n",
        "plt.legend(loc='best', fontsize=14)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Directory to save the plot\n",
        "output_directory = \"path_for_saving_the_feature_importance_bar_graphs\"\n",
        "\n",
        "# Save the plot in PNG and TIFF formats, do not forget 1000 dpi for tiff could take large amount of space in your memory so adjust the resolution properly\n",
        "plt.savefig(f\"{output_directory}/Feature_Importances_results.png\", format='png', dpi=1000)\n",
        "plt.savefig(f\"{output_directory}/Feature_Importances_results.tiff\", format='tiff', dpi=1000)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_TA_RrL49HSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RESIDUALS VISUALIZATION**"
      ],
      "metadata": {
        "id": "f2EW2kMp_rUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Models for which you want to plot, you can add or remove any models that you want\n",
        "model_names = ['XGBoostRegressor', 'Random Forest', 'Bagging', 'Gradient Boosting', 'LightGBMRegressor']\n",
        "\n",
        "# Setting up the figure and axes\n",
        "fig, axs = plt.subplots(nrows=len(model_names), ncols=1, figsize=(8, 6 * len(model_names)))\n",
        "\n",
        "for ax, model_name in zip(axs, model_names):\n",
        "    model = regressors[model_name]\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # Scatter plot without edgecolors\n",
        "    ax.scatter(y_train, y_train_pred, alpha=0.6, s=20, label='Train', color='blue')\n",
        "    ax.scatter(y_test, y_test_pred, alpha=0.6, s=20, label='Test', color='red')\n",
        "\n",
        "    # Line for perfect prediction\n",
        "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--k')\n",
        "\n",
        "    # R^2 score for test and train data\n",
        "    r2_test_value = r2_score(y_test, y_test_pred)\n",
        "    r2_train_value = r2_score(y_train, y_train_pred)\n",
        "    ax.text(0.05, 0.95, f'Test $R^2$: {r2_test_value:.2f}', fontsize=13, transform=ax.transAxes, verticalalignment='top')\n",
        "    ax.text(0.05, 0.85, f'Train $R^2$: {r2_train_value:.2f}', fontsize=13, transform=ax.transAxes, verticalalignment='top')\n",
        "\n",
        "    # Setting title, labels, and legend\n",
        "    ax.set_title(f'Actual vs Predicted for {model_name}\\n', fontsize=15)\n",
        "    ax.set_xlabel('Log (Cellular uptake) - Actual', fontsize=14)\n",
        "    ax.set_ylabel('Log (Cellular uptake) - Predicted', fontsize=14)\n",
        "    ax.legend(loc='lower right', fontsize=12)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "# Directory to save the plot\n",
        "output_directory = \"path_to_saving_directory\"\n",
        "\n",
        "# Save the plot in PNG and TIFF formats\n",
        "plt.savefig(f\"{output_directory}/Actual_vs_Predicted_results.png\", format='png', dpi=1000)\n",
        "plt.savefig(f\"{output_directory}/Actual_vs_Predicted_results.tiff\", format='tiff', dpi=1000)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mSiio5-V_OyM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}